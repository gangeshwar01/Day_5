# In a Jupyter Notebook cell

import numpy as np
import matplotlib.pyplot as plt
from model import Dense, ReLU, MeanSquaredError, SGD

# =============================================================================
# 1. Data Generation
# =============================================================================
print("Generating synthetic dataset...")

def generate_data(n_samples=200):
    """Generates a noisy cubic dataset."""
    X = np.linspace(-5, 5, n_samples).reshape(-1, 1)
    # True function: y = 0.1*x^3 - 0.5*x^2 + x + 5
    y_true = 0.1 * (X**3) - 0.5 * (X**2) + X + 5
    # Add some noise
    noise = np.random.normal(0, 2, (n_samples, 1))
    y = y_true + noise
    return X, y

X_train, y_train = generate_data()

# Plot the generated data
plt.figure(figsize=(8, 6))
plt.scatter(X_train, y_train, alpha=0.6, label='Noisy Data')
plt.title('Synthetic Cubic Dataset')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.grid(True)
plt.show()


# =============================================================================
# 2. Model Definition
# =============================================================================
print("Defining the model architecture...")

# Our network will have one hidden layer with ReLU activation.
# The output layer is a single neuron (linear activation for regression).
input_size = 1
hidden_size = 64
output_size = 1

layer1 = Dense(input_size, hidden_size)
activation1 = ReLU()
layer2 = Dense(hidden_size, output_size)

network = [layer1, activation1, layer2]
loss_function = MeanSquaredError()

# The optimizer will update the parameters of the dense layers
optimizer = SGD(layers=[layer1, layer2], learning_rate=0.001)

# =============================================================================
# 3. Training Loop
# =============================================================================
print("Starting training...")

# Hyperparameters
epochs = 2000
batch_size = 32
loss_history = []

for epoch in range(epochs):
    epoch_loss = 0
    
    # Shuffle data at the beginning of each epoch
    permutation = np.random.permutation(X_train.shape[0])
    X_shuffled = X_train[permutation]
    y_shuffled = y_train[permutation]

    # Mini-batch training
    for i in range(0, X_train.shape[0], batch_size):
        X_batch = X_shuffled[i:i+batch_size]
        y_batch = y_shuffled[i:i+batch_size]
        
        # --- Forward Pass ---
        # Layer 1
        out1 = layer1.forward(X_batch)
        act1 = activation1.forward(out1)
        # Layer 2 (Output)
        y_pred = layer2.forward(act1)
        
        # Calculate loss
        loss = loss_function.forward(y_pred, y_batch)
        epoch_loss += loss

        # --- Backward Pass ---
        # Start backpropagation from the loss gradient
        grad = loss_function.backward(y_pred, y_batch)
        
        # Backprop through Layer 2
        grad = layer2.backward(grad)
        # Backprop through ReLU activation
        grad = activation1.backward(grad)
        # Backprop through Layer 1
        grad = layer1.backward(grad)
        
        # --- Update Weights ---
        optimizer.step()

    # Store average loss for the epoch
    avg_epoch_loss = epoch_loss / (X_train.shape[0] / batch_size)
    loss_history.append(avg_epoch_loss)

    if (epoch + 1) % 200 == 0:
        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_epoch_loss:.4f}')

print("Training finished.")

# =============================================================================
# 4. Visualization
# =============================================================================
print("Visualizing results...")

# --- Plot Loss Curve ---
plt.figure(figsize=(8, 6))
plt.plot(loss_history)
plt.title('Training Loss Curve')
plt.xlabel('Epoch')
plt.ylabel('Mean Squared Error')
plt.grid(True)
plt.show()

# --- Plot Predictions vs. Ground Truth ---
# Generate predictions from the trained model
out1 = layer1.forward(X_train)
act1 = activation1.forward(out1)
y_final_pred = layer2.forward(act1)

plt.figure(figsize=(10, 8))
plt.scatter(X_train, y_train, alpha=0.5, label='Ground Truth')
# Sort values for a smooth line plot
sorted_indices = np.argsort(X_train.flatten())
plt.plot(X_train[sorted_indices], y_final_pred[sorted_indices], color='red', linewidth=3, label='Model Prediction')
plt.title('Model Prediction vs. Ground Truth')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.grid(True)
plt.show()